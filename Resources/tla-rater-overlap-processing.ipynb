{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Segmentation Overlap Processing**\n\nThe below code was used in our data science competition to process overlapping discourse segments and ensure discrete elements.","metadata":{}},{"cell_type":"code","source":"df_sub = pd.read_csv(DATA_PATH/\"prediction_file.csv\")\n\ndef modified(pred_string, diff_words):\n    return len(pred_string) > len(diff_words)\n\ndef get_original(modified, pred_string):\n    if modified:\n        return pred_string\n    else:\n        return \"--\"\n\ndef remove_overlaps(pred_df, by_discourse=False, verbose=False):\n    pred_df['predictionstring'] = [[int(x) for x in pred.split(' ')] for pred in pred_df['predictionstring']]  # Map to integers. Keep as list for better vectorization.\n    pred_df[\"nb_words\"] = pred_df[\"predictionstring\"].map(lambda x: len(x))\n    pred_df[\"start_word\"] = pred_df[\"predictionstring\"].map(lambda x: min(x))  # Some Dataframes already have this information, but computing in case they don't in future\n    # pred_df[\"end_word\"] = pred_df[\"predictionstring\"].map(lambda x: max(x))\n\n    if by_discourse:  # If only removing overlaps within the same discourse element\n        grouped_df = pred_df.groupby([\"id\", \"discourse_type\"])  # Group by essay and discourse type\n    else:\n        grouped_df = pred_df.groupby(\"id\") # Group by essay only\n    \n    processed_groups = []\n    removed_groups = []  # Keep track of cleaned up segments\n    nb_groups = len(grouped_df)\n    pbar = tqdm(total=nb_groups)\n    segments_affected = 0\n    for name, grp in grouped_df:\n        pbar.update(1)\n        grp_sorted = grp.sort_values([\"start_word\",'nb_words'],ascending=True)\n        grp_sorted[\"cum_words\"] = grp_sorted[\"predictionstring\"].cumsum().apply(set)  # Compute cumulative words seen so far\n        grp_sorted[\"cum_words\"] = np.concatenate((np.array([{}]), grp_sorted[\"cum_words\"].iloc[0:-1])) # Shift by 1\n        # Need to shift to consider cumulation up to but excluding current segment.\n        grp_sorted[\"set_diff_words\"] = (grp_sorted[\"predictionstring\"].map(set) - grp_sorted[\"cum_words\"].map(set)).apply(list)  # Compute set difference with current segment in a vectorized way\n        # Reference: https://stackoverflow.com/questions/28457149/how-to-map-a-function-using-multiple-columns-in-pandas -> ListComp faster than apply for some reason.\n        grp_sorted[\"modified\"] = [modified(pred, diff_w) for pred, diff_w in zip(grp_sorted[\"predictionstring\"], grp_sorted[\"set_diff_words\"])]  # Check if set difference is shorter (i.e., there is an overlap) in a vect way\n        grp_sorted[\"noncontiguous\"] = grp_sorted[\"set_diff_words\"].map(lambda x: np.any(np.diff(x) > 1))  # Non-contiguous segments post overlap removal will be deleted\n        grp_sorted[\"nonzero\"] = grp_sorted[\"set_diff_words\"].map(lambda x: len(x) > 0)  # Same for empty segments\n        grp_sorted['original_predstring'] = [get_original(mod, pred) for mod, pred in zip(grp_sorted[\"modified\"], grp_sorted[\"predictionstring\"])]\n        grp_sorted[\"predictionstring\"] = grp_sorted[\"set_diff_words\"].map(lambda x: \" \".join([str(y) for y in x]))\n        # Now filter out\n        grp_processed = grp_sorted[((~grp_sorted[\"noncontiguous\"]) & grp_sorted[\"nonzero\"])]\n        processed_groups.append(grp_processed)\n        grp_removed = grp_sorted[((grp_sorted[\"noncontiguous\"]) | ~grp_sorted[\"nonzero\"])]\n        removed_groups.append(grp_removed)\n        if verbose and len(grp_removed) > 0:\n            print(\"Discarded segments\")\n            print(grp_removed)\n            \n    # End of all group for loops\n    new_segments = pd.concat(processed_groups)[[\"id\", \"discourse_type\", \"score_discourse_effectiveness_0\", \n                                               \"score_discourse_effectiveness_1\", \n                                               \"predictionstring\", \"modified\", \"original_predstring\"]].to_csv(\"processed.csv\")\n    removed_segments = pd.concat(removed_groups)[[\"id\", \"discourse_type\", \"score_discourse_effectiveness_0\", \n                                               \"score_discourse_effectiveness_1\", \n                                                 \"predictionstring\", \"modified\", \"original_predstring\"]].to_csv(\"removed.csv\")\n    if verbose:\n        print(str(segments_affected) + \" segments have been modified/removed by pre-processing\")\n    pbar.close()\n\nremove_overlaps(df_sub, by_discourse=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}