{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9309936,"sourceType":"datasetVersion","datasetId":3710330}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Single Model Inference (deberta-large)\n\n#### **Private Test Set:**\nFB1 metric: **0.7304637538407162**","metadata":{}},{"cell_type":"code","source":"import codecs\nfrom text_unidecode import unidecode\n\ndef replace_encoding_with_utf8(error):\n    return error.object[error.start : error.end].encode(\"utf-8\"), error.end\n\n\ndef replace_decoding_with_cp1252(error):\n    return error.object[error.start : error.end].decode(\"cp1252\"), error.end\n\n\n# Register the encoding and decoding error handlers for `utf-8` and `cp1252`.\ncodecs.register_error(\"replace_encoding_with_utf8\", replace_encoding_with_utf8)\ncodecs.register_error(\"replace_decoding_with_cp1252\", replace_decoding_with_cp1252)\n\ndef resolve_encodings_and_normalize(text):\n    \"\"\"Resolve the encoding problems and normalize the abnormal characters.\"\"\"\n    text = (\n        text.encode(\"raw_unicode_escape\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n        .encode(\"cp1252\", errors=\"replace_encoding_with_utf8\")\n        .decode(\"utf-8\", errors=\"replace_decoding_with_cp1252\")\n    )\n    text = unidecode(text)\n    return text\n\ndef clean_text(text):\n    text = text.replace(u'\\xa0', u' ')\n    text = text.replace(u'\\x85', u'\\n')\n    text = text.strip()\n    text = resolve_encodings_and_normalize(text)\n\n    return text","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-03T14:43:10.308546Z","iopub.execute_input":"2024-09-03T14:43:10.308833Z","iopub.status.idle":"2024-09-03T14:43:10.332172Z","shell.execute_reply.started":"2024-09-03T14:43:10.308807Z","shell.execute_reply":"2024-09-03T14:43:10.331496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport re\nimport numpy as np\nimport random\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, tokenizer, cfg):\n        \n        self.texts = texts\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, index):\n\n        text = clean_text(self.texts[index])\n\n        tokens = self.tokenizer(\n            text,\n            add_special_tokens = True,\n            return_offsets_mapping=True\n            )\n        \n        input_ids = torch.LongTensor(tokens['input_ids'])\n        attention_mask = torch.LongTensor(tokens['attention_mask'])\n        offset_mapping = np.array(tokens['offset_mapping'])\n        offset_mapping = self.strip_offset_mapping(text, offset_mapping)\n\n        # token slices of words\n        woff = self.get_word_offsets(text)\n        toff = offset_mapping\n        wx1, wx2 = woff.T\n        tx1, tx2 = toff.T\n        ix1 = np.maximum(wx1[..., None], tx1[None, ...])\n        ix2 = np.minimum(wx2[..., None], tx2[None, ...])\n        ux1 = np.minimum(wx1[..., None], tx1[None, ...])\n        ux2 = np.maximum(wx2[..., None], tx2[None, ...])\n        ious = (ix2 - ix1).clip(min=0) / (ux2 - ux1 + 1e-12)\n        assert (ious > 0).any(-1).all()\n\n        word_boxes = []\n        for row in ious:\n            inds = row.nonzero()[0]\n            word_boxes.append([inds[0], 0, inds[-1] + 1, 1])\n        word_boxes = torch.FloatTensor(word_boxes)\n\n        return dict(text=text, input_ids=input_ids, attention_mask=attention_mask, word_boxes=word_boxes)\n\n    def strip_offset_mapping(self, text, offset_mapping):\n        ret = []\n        for start, end in offset_mapping:\n            match = list(re.finditer('\\S+', text[start:end]))\n            if len(match) == 0:\n                ret.append((start, end))\n            else:\n                span_start, span_end = match[0].span()\n                ret.append((start + span_start, start + span_end))\n        return np.array(ret)\n\n    def get_word_offsets(self, text):\n        matches = re.finditer(\"\\S+\", text)\n        spans = []\n        words = []\n        for match in matches:\n            span = match.span()\n            word = match.group()\n            spans.append(span)\n            words.append(word)\n        assert tuple(words) == tuple(text.split())\n        return np.array(spans)\n    \nclass CustomCollator(object):\n    def __init__(self, pad_token_id):\n        self.pad_token_id = pad_token_id\n\n    def __call__(self, samples):\n        batch_size = len(samples)\n        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n\n        sample = samples[0]\n        \n        max_seq_length = len(sample['input_ids'])\n        padded_length = max_seq_length\n\n        input_shape = (1, padded_length)\n        input_ids = torch.full(input_shape,\n                               self.pad_token_id,\n                               dtype=torch.long)\n        attention_mask = torch.zeros(input_shape, dtype=torch.long)\n\n        seq_length = len(sample['input_ids'])\n        input_ids[0, :seq_length] = sample['input_ids']\n        attention_mask[0, :seq_length] = sample['attention_mask']\n\n        text = sample['text']\n        word_boxes = sample['word_boxes']\n\n        return dict(text=text,\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    word_boxes=word_boxes)\n    \nclass TextDataModule(pl.LightningDataModule):\n    def __init__(\n        self,\n        texts = None,\n        tokenizer = None,\n        cfg = None,\n    ):\n        super().__init__()\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.cfg = cfg\n\n    def setup(self, stage):\n        if stage == 'predict':\n            self.predict_dataset = TextDataset(self.texts, self.tokenizer, self.cfg)\n        else:\n            raise Exception()\n\n    def predict_dataloader(self):\n        custom_collator = CustomCollator(self.tokenizer.pad_token_id)\n        return DataLoader(self.predict_dataset, **self.cfg[\"val_loader\"], collate_fn=custom_collator)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:43:10.3517Z","iopub.execute_input":"2024-09-03T14:43:10.351938Z","iopub.status.idle":"2024-09-03T14:43:25.808989Z","shell.execute_reply.started":"2024-09-03T14:43:10.351916Z","shell.execute_reply":"2024-09-03T14:43:25.808173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom transformers import get_polynomial_decay_schedule_with_warmup, get_cosine_schedule_with_warmup\n\nimport pandas as pd\nimport yaml\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.ops import roi_align, nms\n\ndef aggregate_tokens_to_words(feat, word_boxes):\n    feat = feat.permute(0, 2, 1).unsqueeze(2)\n    output = roi_align(feat, [word_boxes], 1, aligned=True)\n    return output.squeeze(-1).squeeze(-1)\n\ndef span_nms(start, end, score, nms_thr):\n    boxes = torch.stack(\n        [\n            start,\n            torch.zeros_like(start),\n            end,\n            torch.ones_like(start),\n        ],\n        dim=1,\n    ).float()\n    keep = nms(boxes, score, nms_thr)\n    return keep\n\ndef get_pred(col):\n    def row_wise(row):\n        return \" \".join([str(x) for x in range(row.start,row.end)])\n    return row_wise\n\ndef make_preds(preds, ids, th=0.303):\n\n    r = []\n\n    for text_id, (obj_pred, reg_pred, cls_pred, eff_pred) in zip(ids, preds):\n\n        obj_pred = obj_pred.sigmoid()\n        reg_pred = reg_pred.exp()\n        cls_pred = cls_pred.sigmoid()\n        eff_pred = eff_pred.softmax(-1)\n\n        obj_scores = obj_pred\n        cls_scores, cls_labels = cls_pred.max(-1)\n        eff_scores = eff_pred\n        pr_scores = (obj_scores * cls_scores)**0.5\n\n        pos_inds = pr_scores > 0.5\n\n        if pos_inds.sum() == 0:\n            continue\n        \n        pr_score, pr_label, pr_eff = pr_scores[pos_inds], cls_labels[pos_inds], eff_scores[pos_inds]\n        pos_loc = pos_inds.nonzero().flatten()\n        start = pos_loc - reg_pred[pos_inds, 0]\n        end = pos_loc + reg_pred[pos_inds, 1]\n\n        min_idx, max_idx = 0, obj_pred.numel() - 1\n        start = start.clamp(min=min_idx, max=max_idx).round().long()\n        end = end.clamp(min=min_idx, max=max_idx).round().long()\n\n        # nms\n        keep = span_nms(start, end, pr_score, th)\n        start = start[keep]\n        end = end[keep]\n        pr_score = pr_score[keep]\n        pr_label = pr_label[keep]\n        pr_eff = pr_eff[keep]\n\n        res = dict(\n            id=text_id,\n            start=start.cpu().numpy(),\n            end=end.cpu().numpy(),\n            score_discourse_type=pr_score.cpu().numpy(),\n            discourse_type=pr_label.cpu().numpy(),\n            score_discourse_effectiveness_0 = pr_eff[:,0].cpu().numpy() + pr_eff[:,2].cpu().numpy(),\n            score_discourse_effectiveness_1 = pr_eff[:,1].cpu().numpy(),\n        )\n\n        res = pd.DataFrame(res).sort_values('start').reset_index(drop=True)\n        res['predictionstring'] = res.apply(get_pred(' '),axis=1)\n        \n        r.append(res)\n\n    return pd.concat(r,axis=0).reset_index(drop=True)\n\nclass TextModel(pl.LightningModule):\n    def __init__(self, cfg, config_path=None):\n\n        super().__init__()\n        self.cfg = cfg\n\n        model_cfg = cfg['model']\n\n        self.config = torch.load(config_path)    \n        self.backbone = AutoModel.from_config(self.config)\n\n        hidden_size = self.config.hidden_size\n        self.fc = nn.Linear(hidden_size, 1+2+7+3)\n        \n    def forward(self, inputs):\n        x = self.backbone(**inputs).last_hidden_state\n        x = self.fc(x)\n        return x\n\n    def forward_logits(self, data):\n\n        batch_size = data['input_ids'].size(0)\n        assert batch_size == 1, f'Only batch_size=1 supported, got batch_size={batch_size}.'\n        \n        inputs = {\n            'input_ids': data['input_ids'],\n            'attention_mask': data['attention_mask'],\n        }\n        \n        logits = self(inputs)\n\n        logits = aggregate_tokens_to_words(logits, data['word_boxes'])\n        assert logits.size(0) == data['text'].split().__len__()\n\n        obj_pred = logits[..., 0]\n        reg_pred = logits[..., 1:3]\n        cls_pred = logits[..., 3:-3]\n        eff_pred = logits[..., -3:]\n        \n        return obj_pred, reg_pred, cls_pred, eff_pred\n\n    def predict_step(self, data, batch_idx):\n        obj_pred, reg_pred, cls_pred, eff_pred = self.forward_logits(data)\n\n        return obj_pred.detach().cpu(), reg_pred.detach().cpu(), cls_pred.detach().cpu(), eff_pred.detach().cpu()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:43:25.810477Z","iopub.execute_input":"2024-09-03T14:43:25.810779Z","iopub.status.idle":"2024-09-03T14:43:25.838465Z","shell.execute_reply.started":"2024-09-03T14:43:25.810753Z","shell.execute_reply":"2024-09-03T14:43:25.837611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, datamodule, ids):\n    preds = trainer.predict(model, datamodule=datamodule)\n    pred_df = make_preds(preds, ids, 0.303)\n    \n    return pred_df\n\ndef predict_single(model, text):\n    datamodule = TextDataModule(texts=[text], tokenizer=tokenizer, cfg=cfg)\n\n    preds = trainer.predict(model, datamodule=datamodule)\n    pred_df = make_preds(preds, ['NO_ID'], 0.303)\n    \n    return pred_df","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:43:25.83956Z","iopub.execute_input":"2024-09-03T14:43:25.839913Z","iopub.status.idle":"2024-09-03T14:43:25.858473Z","shell.execute_reply.started":"2024-09-03T14:43:25.839881Z","shell.execute_reply":"2024-09-03T14:43:25.8577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/tlab-dataset/tokenizer/tokenizer')\nconfig_path = '/kaggle/input/tlab-dataset/config.pth'\n\nwith open('/kaggle/input/tlab-dataset/deberta_large_025.yml', 'r') as f:\n    cfg = yaml.safe_load(f)\n    \ntrainer = pl.Trainer(logger=False, **cfg['trainer'])\n\nmodel = TextModel(cfg, config_path)\n\nstate_dict = torch.load('/kaggle/input/tlab-dataset/best_deberta_large.ckpt')['state_dict']\n\nfor key in list(state_dict.keys()):\n    state_dict[key] = state_dict.pop(key)\n\nmodel.load_state_dict(state_dict, strict=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:43:25.860329Z","iopub.execute_input":"2024-09-03T14:43:25.860652Z","iopub.status.idle":"2024-09-03T14:43:43.962171Z","shell.execute_reply.started":"2024-09-03T14:43:25.860565Z","shell.execute_reply":"2024-09-03T14:43:43.961214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tlab-dataset/persuade_corpus.csv', low_memory=False)\ndf = df[df['test_split_feedback_1'] == 'Private'].reset_index(drop=True)\ndf = df[['essay_id', 'full_text']].drop_duplicates('essay_id').reset_index(drop=True)\n\ntexts = df['full_text'].values.tolist()[:10]\nids = df['essay_id'].values.tolist()[:10]\n\ndatamodule = TextDataModule(texts=texts, tokenizer=tokenizer, cfg=cfg)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:43:43.963542Z","iopub.execute_input":"2024-09-03T14:43:43.964213Z","iopub.status.idle":"2024-09-03T14:44:01.228269Z","shell.execute_reply.started":"2024-09-03T14:43:43.964183Z","shell.execute_reply":"2024-09-03T14:44:01.227425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = predict(model, datamodule, ids)\npred_df","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:44:01.229418Z","iopub.execute_input":"2024-09-03T14:44:01.22975Z","iopub.status.idle":"2024-09-03T14:44:05.346132Z","shell.execute_reply.started":"2024-09-03T14:44:01.229721Z","shell.execute_reply":"2024-09-03T14:44:05.34509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = predict_single(model, texts[0])\npred_df","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:44:05.347507Z","iopub.execute_input":"2024-09-03T14:44:05.347826Z","iopub.status.idle":"2024-09-03T14:44:07.883878Z","shell.execute_reply.started":"2024-09-03T14:44:05.347799Z","shell.execute_reply":"2024-09-03T14:44:07.882839Z"},"trusted":true},"execution_count":null,"outputs":[]}]}